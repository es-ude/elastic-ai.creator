{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from elasticai.creator.nn.fixed_point._math_operations import MathOperations\n",
    "from elasticai.creator.nn.fixed_point._two_complement_fixed_point_config import FixedPointConfig\n",
    "from elasticai.creator.vhdl.testbench_helper import tensor_to_vhdl_vector\n",
    "from torch import nn, Tensor\n",
    "\n",
    "# Creator imports\n",
    "from elasticai.creator.nn import Sequential\n",
    "from elasticai.creator.nn.fixed_point import Linear, Tanh\n",
    "from package.dnn.pytorch_handler import __model_settings_common\n",
    "from package.dnn.pytorch_handler import ModelRegistry"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "models_available = ModelRegistry()",
   "id": "a90e634a3d16dc4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_bits = 8\n",
    "frac_bits = 5\n",
    "bn_affine = True\n",
    "num_steps = 2**8\n",
    "interval = (-4, 3.96875)"
   ],
   "id": "9c297a410cbbc7c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@models_available.register\n",
    "class dnn_ae_v1_Q_wo_BN(__model_settings_common):\n",
    "    \"\"\"Class of an autoencoder with Dense-Layer for feature extraction\"\"\"\n",
    "    def __init__(self, input_size=32, output_size=3):\n",
    "        super().__init__('Autoencoder')\n",
    "        self.model_shape = (1, input_size)\n",
    "        self.model_embedded = True\n",
    "        iohiddenlayer = [input_size, 20, 14, output_size]\n",
    "        do_train_bias = True\n",
    "        do_train_batch = True\n",
    "\n",
    "        # --- Encoder Path\n",
    "        self.encoder = Sequential(\n",
    "            Linear(in_features=iohiddenlayer[0], out_features=iohiddenlayer[1], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=False),\n",
    "            Tanh(total_bits=total_bits, frac_bits=frac_bits, num_steps=num_steps, sampling_intervall=interval),\n",
    "            Linear(in_features=iohiddenlayer[1], out_features=iohiddenlayer[2], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=False),\n",
    "            Tanh(total_bits=total_bits, frac_bits=frac_bits, num_steps=num_steps, sampling_intervall=interval),\n",
    "            Linear(in_features=iohiddenlayer[2], out_features=iohiddenlayer[3], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=True),\n",
    "        )\n",
    "        # --- Decoder Path\n",
    "        self.decoder = Sequential(\n",
    "            Tanh(total_bits=total_bits, frac_bits=frac_bits, num_steps=num_steps),\n",
    "            Linear(in_features=iohiddenlayer[3], out_features=iohiddenlayer[2], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=True),\n",
    "            Tanh(total_bits=total_bits, frac_bits=frac_bits, num_steps=num_steps, sampling_intervall=interval),\n",
    "            Linear(in_features=iohiddenlayer[2], out_features=iohiddenlayer[1], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=False),\n",
    "            Tanh(total_bits=total_bits, frac_bits=frac_bits, num_steps=num_steps, sampling_intervall=interval),\n",
    "            Linear(in_features=iohiddenlayer[1], out_features=iohiddenlayer[0], bias=do_train_bias, total_bits=total_bits, frac_bits=frac_bits, parallel=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> [Tensor, Tensor]:\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded, self.decoder(encoded)\n",
    "    \n",
    "    def forward_first_layer(self, x: Tensor) -> Tensor:\n",
    "        return self.encoder[0](x)\n",
    "    \n",
    "    def create_design(self, name):\n",
    "        encoder = self.encoder.create_design(f\"{name}_encoder\")\n",
    "        decoder = self.decoder.create_design(f\"{name}_decoder\")\n",
    "        return encoder, decoder"
   ],
   "id": "4f67a6dd3cd71f89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "autoencoder = torch.load(f=\"/home/silas/PycharmProjects/denspp.offline/3_Python/runs/20250127_122436_train_dnn_ae_v1_Q_wo_BN_ae/model_ae_fold000_epoch0028.pth\")",
   "id": "96f7a1615694e839"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = FixedPointConfig(total_bits, frac_bits)\n",
    "operations = MathOperations(config)"
   ],
   "id": "f57fc180e935517f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def float_to_signed_int(value: float | list) -> int | list:\n",
    "    if isinstance(value, list):\n",
    "        return list(map(float_to_signed_int, value))\n",
    "    return config.as_integer(value)\n",
    "\n",
    "def flatten(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):  # Check if the item is a list\n",
    "            result.extend(flatten(item))  # Recursively flatten sublists\n",
    "        elif isinstance(item, (int, float)):  # Ensure it's a number\n",
    "            result.append(item)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported data type: {type(item)} in list\")\n",
    "    return result\n",
    "\n",
    "def get_weights_of_encoder(w_layers: list[int], model):\n",
    "    layers = [model.encoder[x] for x in w_layers]\n",
    "    \n",
    "    weights_list = []\n",
    "\n",
    "    for layer in layers:\n",
    "        bias = [0] * layer.out_features if layer.bias is None else operations.quantize(layer.bias).tolist()\n",
    "        weights = operations.quantize(layer.weight).tolist()\n",
    "    \n",
    "        weights_list.append(bias)\n",
    "        weights_list.append(weights)\n",
    "\n",
    "    weight_list = flatten(weights_list)\n",
    "    return weight_list\n",
    "\n",
    "weight_tensor = torch.tensor(get_weights_of_encoder([0,2,4], autoencoder))\n",
    "tensor_to_vhdl_vector(weight_tensor, config, as_matrix=True)"
   ],
   "id": "b077e126294aa080"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
